Failure # 1 (occurred at 2023-10-19_15-46-34)
Task was killed due to the node running low on memory.
Memory on the node (IP: 171.65.93.126, ID: cc897eb4185284b90cd8f39a6d5255570518d026bbe34379dbabfdd7) where the task (task ID: ffffffffffffffff538c5345e8fbd247bea7d8f201000000, name=ImplicitFunc.__init__, pid=15449, memory used=0.10GB) was running was 59.73GB / 62.81GB (0.950853), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd66d508cf889ab42830fec4b877ef338b94f0cdb76a6b2fcececfb5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 171.65.93.126`. To see the logs of the worker, use `ray logs worker-dd66d508cf889ab42830fec4b877ef338b94f0cdb76a6b2fcececfb5*out -ip 171.65.93.126. Top 10 memory users:
PID	MEM(GB)	COMMAND
5631	2.78	/home/nrubio/anaconda3/bin/python /home/nrubio/anaconda3/bin/conda install -c conda-forge hydra
27856	1.41	ray::ImplicitFunc.train
2492	1.40	ray::ImplicitFunc.train
28015	1.40	ray::ImplicitFunc.train
2586	1.40	ray::ImplicitFunc.train
3074	1.36	python3 util/regression/neural_network/launch_training.py
7201	1.36	python3 util/regression/neural_network/launch_training.py
3825	1.36	python3 util/regression/neural_network/launch_training.py
22794	1.36	python3 util/regression/neural_network/launch_training.py
22204	1.36	python3 util/regression/neural_network/launch_training.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
