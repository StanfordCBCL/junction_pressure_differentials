Failure # 1 (occurred at 2023-10-19_15-46-36)
Task was killed due to the node running low on memory.
Memory on the node (IP: 171.65.93.126, ID: cc897eb4185284b90cd8f39a6d5255570518d026bbe34379dbabfdd7) where the task (task ID: fffffffffffffffff762923211d8a4ba6a69061b01000000, name=ImplicitFunc.__init__, pid=15550, memory used=0.10GB) was running was 59.67GB / 62.81GB (0.950026), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4fe29da6f73e67505b986988d931de1393e5649b13234d5202bbbf4e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 171.65.93.126`. To see the logs of the worker, use `ray logs worker-4fe29da6f73e67505b986988d931de1393e5649b13234d5202bbbf4e*out -ip 171.65.93.126. Top 10 memory users:
PID	MEM(GB)	COMMAND
5631	2.78	/home/nrubio/anaconda3/bin/python /home/nrubio/anaconda3/bin/conda install -c conda-forge hydra
27856	1.41	ray::ImplicitFunc.train
2492	1.40	ray::ImplicitFunc.train
28015	1.40	ray::ImplicitFunc.train
2586	1.40	ray::ImplicitFunc.train
3074	1.36	python3 util/regression/neural_network/launch_training.py
7201	1.36	python3 util/regression/neural_network/launch_training.py
3825	1.36	python3 util/regression/neural_network/launch_training.py
22794	1.36	python3 util/regression/neural_network/launch_training.py
22204	1.36	python3 util/regression/neural_network/launch_training.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
